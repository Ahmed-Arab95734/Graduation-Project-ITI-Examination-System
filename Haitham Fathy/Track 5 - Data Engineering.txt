-- Department: System Development, Track: Data Engineering, Course: Data Engineering Intro with Python
INSERT INTO Question_Bank (Question_ID, Course_ID, Question_Type, Question_Description, Question_Model_Answer) VALUES (2000, 29, N'MCQ', N'Which Python library is fundamental for numerical computing?', N'NumPy'), (2001, 29, N'True/False', N'Pandas DataFrames are mutable in size.', N'False'), (2002, 29, N'MCQ', N'What is the primary data structure in Pandas for data analysis?', N'DataFrame'), (2003, 29, N'MCQ', N'Which file format is row-based and human-readable?', N'JSON'), (2004, 29, N'True/False', N'Data Engineering is primarily concerned with building machine learning models.', N'False'), (2005, 29, N'MCQ', N'What is the process of moving data from a source to a destination called?', N'Data Pipeline'), (2006, 29, N'MCQ', N'Which library is commonly used for making HTTP requests in Python?', N'requests'), (2007, 29, N'True/False', N'A CSV file stores data in a columnar format.', N'False'), (2008, 29, N'MCQ', N'What does API stand for?', N'Application Programming Interface'), (2009, 29, N'MCQ', N'Which of these is a key responsibility of a Data Engineer?', N'Building reliable data pipelines'), (2010, 29, N'True/False', N'The `loc` method in Pandas selects data by numerical index.', N'False'), (2011, 29, N'MCQ', N'What function in Pandas is used to read a CSV file?', N'pd.read_csv()'), (2012, 29, N'True/False', N'Virtual environments are used to manage project-specific dependencies in Python.', N'True'), (2013, 29, N'MCQ', N'Which concept refers to cleaning and transforming raw data?', N'Data Wrangling'), (2014, 29, N'MCQ', N'What is the standard package manager for Python?', N'pip');

-- Department: System Development, Track: Data Engineering, Course: Advanced SQL & Data Warehousing
INSERT INTO Question_Bank (Question_ID, Course_ID, Question_Type, Question_Description, Question_Model_Answer) VALUES (2015, 30, N'MCQ', N'Which SQL function is an example of a window function?', N'ROW_NUMBER()'), (2016, 30, N'True/False', N'A star schema consists of one or more fact tables referencing any number of dimension tables.', N'True'), (2017, 30, N'MCQ', N'What does CTE stand for in SQL?', N'Common Table Expression'), (2018, 30, N'MCQ', N'Which type of table in a data warehouse stores measurements or metrics?', N'Fact Table'), (2019, 30, N'True/False', N'OLTP systems are optimized for complex data analysis.', N'False'), (2020, 30, N'MCQ', N'A schema where dimension tables are normalized into multiple related tables is called?', N'Snowflake Schema'), (2021, 30, N'MCQ', N'What SQL clause is used to filter results of a window function?', N'QUALIFY'), (2022, 30, N'True/False', N'A dimension table contains descriptive attributes.', N'True'), (2023, 30, N'MCQ', N'What is a surrogate key?', N'A system-generated unique identifier'), (2024, 30, N'MCQ', N'The process of storing historical data changes in a dimension table is called?', N'Slowly Changing Dimension (SCD)'), (2025, 30, N'True/False', N'A primary key cannot contain NULL values.', N'True'), (2026, 30, N'MCQ', N'Which `JOIN` returns all rows when there is a match in either table?', N'FULL OUTER JOIN'), (2027, 30, N'True/False', N'A view in SQL is a virtual table based on the result-set of an SQL statement.', N'True'), (2028, 30, N'MCQ', N'Which of these is a type of data model?', N'Relational Model'), (2029, 30, N'MCQ', N'What does OLAP stand for?', N'Online Analytical Processing');

-- Department: System Development, Track: Data Engineering, Course: ETL, Hadoop & Spark
INSERT INTO Question_Bank (Question_ID, Course_ID, Question_Type, Question_Description, Question_Model_Answer) VALUES (2030, 31, N'MCQ', N'What does ETL stand for?', N'Extract, Transform, Load'), (2031, 31, N'True/False', N'In an ELT process, data is transformed before being loaded into the data warehouse.', N'False'), (2032, 31, N'MCQ', N'Which component of Hadoop is responsible for storing data?', N'HDFS'), (2033, 31, N'MCQ', N'What is the core data structure in Spark since version 2.x?', N'DataFrame'), (2034, 31, N'True/False', N'Spark processes data faster than Hadoop MapReduce primarily because it uses in-memory processing.', N'True'), (2035, 31, N'MCQ', N'What is the resource manager in a Hadoop ecosystem?', N'YARN'), (2036, 31, N'MCQ', N'The principle of "lazy evaluation" in Spark means that...?', N'Transformations are not executed until an action is called'), (2037, 31, N'True/False', N'An RDD (Resilient Distributed Dataset) in Spark is mutable.', N'False'), (2038, 31, N'MCQ', N'Which of these is a Spark "action"?', N'count()'), (2039, 31, N'MCQ', N'HDFS is designed for...?', N'Storing large files'), (2040, 31, N'True/False', N'MapReduce is a programming model for processing large data sets.', N'True'), (2041, 31, N'MCQ', N'What is the role of the Driver Program in Spark?', N'To coordinate workers and execute the main function'), (2042, 31, N'True/False', N'Hadoop is a single monolithic piece of software.', N'False'), (2043, 31, N'MCQ', N'Which of these is a Spark "transformation"?', N'filter()'), (2044, 31, N'MCQ', N'The "T" in ELT stands for...?', N'Transform');

-- Department: System Development, Track: Data Engineering, Course: Cloud Data Platforms & Data Lakes
INSERT INTO Question_Bank (Question_ID, Course_ID, Question_Type, Question_Description, Question_Model_Answer) VALUES (2045, 32, N'MCQ', N'A central repository for raw, unstructured data at any scale is called a...?', N'Data Lake'), (2046, 32, N'True/False', N'A Data Warehouse typically stores structured and processed data.', N'True'), (2047, 32, N'MCQ', N'Which of these is an example of a columnar storage format?', N'Parquet'), (2048, 32, N'MCQ', N'Which AWS service is commonly used as the storage layer for a data lake?', N'Amazon S3'), (2049, 32, N'True/False', N'Schema-on-read is a characteristic of a Data Warehouse.', N'False'), (2050, 32, N'MCQ', N'Which Google Cloud service is a serverless, highly scalable data warehouse?', N'Google BigQuery'), (2051, 32, N'MCQ', N'What is a major benefit of columnar file formats like Parquet?', N'Improved query performance and compression'), (2052, 32, N'True/False', N'Data Lakes are generally more flexible and less expensive than traditional Data Warehouses.', N'True'), (2053, 32, N'MCQ', N'Which Azure service is a distributed data storage and analytics service?', N'Azure Data Lake Storage'), (2054, 32, N'MCQ', N'What is the term for the metadata definition of data?', N'Schema'), (2055, 32, N'True/False', N'It is impossible to run SQL queries on data in a data lake.', N'False'), (2056, 32, N'MCQ', N'Which of the following is an open-source columnar file format?', N'ORC'), (2057, 32, N'True/False', N'Data governance is less important in a data lake than in a data warehouse.', N'False'), (2058, 32, N'MCQ', N'What does "serverless" mean in the context of services like BigQuery?', N'Users do not manage the underlying infrastructure'), (2059, 32, N'MCQ', N'Which AWS service is a petabyte-scale data warehouse?', N'Amazon Redshift');

-- Department: System Development, Track: Data Engineering, Course: Real-time Streaming with Kafka
INSERT INTO Question_Bank (Question_ID, Course_ID, Question_Type, Question_Description, Question_Model_Answer) VALUES (2060, 33, N'MCQ', N'What is Apache Kafka?', N'A distributed streaming platform'), (2061, 33, N'True/False', N'In Kafka, a message is called a "record".', N'True'), (2062, 33, N'MCQ', N'A stream of records in Kafka is organized into a...?', N'Topic'), (2063, 33, N'MCQ', N'What is the role of a Kafka "Broker"?', N'To store data'), (2064, 33, N'True/False', N'A Kafka Producer writes data to topics.', N'True'), (2065, 33, N'MCQ', N'How does Kafka achieve fault tolerance and scalability for topics?', N'Partitions'), (2066, 33, N'MCQ', N'What is the purpose of Zookeeper in a Kafka cluster?', N'To manage cluster metadata and broker coordination'), (2067, 33, N'True/False', N'A single partition in Kafka can be consumed by multiple consumers in the same consumer group.', N'False'), (2068, 33, N'MCQ', N'A Kafka "Consumer" subscribes to one or more...?', N'Topics'), (2069, 33, N'MCQ', N'What is the unique identifier for a record within a partition called?', N'Offset'), (2070, 33, N'True/False', N'Kafka messages are deleted immediately after they are consumed.', N'False'), (2071, 33, N'MCQ', N'What ensures that consumers in a group do not read the same message?', N'Consumer Group'), (2072, 33, N'True/False', N'Kafka is designed for batch processing, not real-time streaming.', N'False'), (2073, 33, N'MCQ', N'What is a key benefit of using Kafka?', N'High throughput and low latency'), (2074, 33, N'MCQ', N'A set of brokers in Kafka is called a...?', N'Cluster');

-- Department: System Development, Track: Data Engineering, Course: Workflow Orchestration with Airflow
INSERT INTO Question_Bank (Question_ID, Course_ID, Question_Type, Question_Description, Question_Model_Answer) VALUES (2075, 34, N'MCQ', N'In Airflow, a workflow is represented as a...?', N'DAG'), (2076, 34, N'True/False', N'An Airflow DAG file is written in YAML.', N'False'), (2077, 34, N'MCQ', N'What does DAG stand for?', N'Directed Acyclic Graph'), (2078, 34, N'MCQ', N'A single unit of work in a DAG is called a...?', N'Task'), (2079, 34, N'True/False', N'A task in Airflow is an instance of an Operator.', N'True'), (2080, 34, N'MCQ', N'Which Airflow component is responsible for scheduling DAG runs?', N'Scheduler'), (2081, 34, N'MCQ', N'What is the concept of a task producing the same output given the same input?', N'Idempotency'), (2082, 34, N'True/False', N'In a DAG, it is possible to have a circular dependency between tasks.', N'False'), (2083, 34, N'MCQ', N'Which operator would you use to run a Python function in Airflow?', N'PythonOperator'), (2084, 34, N'MCQ', N'The process of running a DAG for a past date is called...?', N'Backfilling'), (2085, 34, N'True/False', N'Airflow is primarily a data streaming tool.', N'False'), (2086, 34, N'MCQ', N'What is an XCom in Airflow?', N'A mechanism for tasks to pass messages'), (2087, 34, N'True/False', N'The Airflow Webserver is used to execute tasks.', N'False'), (2088, 34, N'MCQ', N'Which of these defines the relationship between tasks in a DAG?', N'Dependencies'), (2089, 34, N'MCQ', N'What is a predefined, reusable task template in Airflow?', N'Operator');

-- Department: System Development, Track: Data Engineering, Course: Data Engineering Capstone Project
INSERT INTO Question_Bank (Question_ID, Course_ID, Question_Type, Question_Description, Question_Model_Answer) VALUES (2090, 35, N'MCQ', N'What is the first step in designing a data pipeline?', N'Understanding requirements and data sources'), (2091, 35, N'True/False', N'A capstone project is designed to test knowledge of a single, isolated tool.', N'False'), (2092, 35, N'MCQ', N'Why is data quality monitoring important in a pipeline?', N'To ensure data is accurate, complete, and reliable'), (2093, 35, N'MCQ', N'Which of the following is a critical non-functional requirement for a data pipeline?', N'Scalability'), (2094, 35, N'True/False', N'Hardcoding credentials in your pipeline code is a secure practice.', N'False'), (2095, 35, N'MCQ', N'What is the purpose of a staging area in an ETL process?', N'To temporarily store data during transformation'), (2096, 35, N'MCQ', N'When choosing a data storage solution, a key factor is...?', N'The data model and query patterns'), (2097, 35, N'True/False', N'For a project, it''s best to choose the newest technology available, regardless of the use case.', N'False'), (2098, 35, N'MCQ', N'Which concept involves tracking the lineage and movement of data?', N'Data Governance'), (2099, 35, N'MCQ', N'What is a common challenge in data engineering projects?', N'Handling evolving data schemas'), (2100, 35, N'True/False', N'Automated testing is not necessary for data pipelines.', N'False'), (2101, 35, N'MCQ', N'A good project design should be...?', N'Modular and maintainable'), (2102, 35, N'True/False', N'Documentation is an optional part of a capstone project.', N'False'), (2103, 35, N'MCQ', N'What does it mean for a pipeline to be "resilient"?', N'It can recover from failures'), (2104, 35, N'MCQ', N'Why is logging important in a data pipeline?', N'For debugging and monitoring');



-- Department: System Development, Track: Data Engineering, Course: Data Engineering Intro with Python
INSERT INTO Question_Choice (Question_Choice_ID, Question_ID, Choice_Text) VALUES (2000, 2000, N'Pandas'), (2001, 2000, N'Matplotlib'), (2002, 2000, N'NumPy'), (2003, 2000, N'Requests'), (2004, 2002, N'Series'), (2005, 2002, N'DataFrame'), (2006, 2002, N'Array'), (2007, 2002, N'List'), (2008, 2003, N'Parquet'), (2009, 2003, N'ORC'), (2010, 2003, N'Avro'), (2011, 2003, N'JSON'), (2012, 2005, N'Data Pipeline'), (2013, 2005, N'Data Model'), (2014, 2005, N'Data Lake'), (2015, 2005, N'Data Mart'), (2016, 2006, N'pandas'), (2017, 2006, N'numpy'), (2018, 2006, N'requests'), (2019, 2006, N'airflow'), (2020, 2008, N'Advanced Programming Interface'), (2021, 2008, N'Application Programming Interface'), (2022, 2008, N'Automated Protocol Interaction'), (2023, 2008, N'Application Protocol Interchange'), (2024, 2009, N'Visualizing data'), (2025, 2009, N'Building machine learning models'), (2026, 2009, N'Building reliable data pipelines'), (2027, 2009, N'Presenting results to stakeholders'), (2028, 2011, N'pd.read_csv()'), (2029, 2011, N'pd.open_csv()'), (2030, 2011, N'pd.load_csv()'), (2031, 2011, N'pd.get_csv()'), (2032, 2013, N'Data Governance'), (2033, 2013, N'Data Science'), (2034, 2013, N'Data Wrangling'), (2035, 2013, N'Data Modeling'), (2036, 2014, N'conda'), (2037, 2014, N'pip'), (2038, 2014, N'npm'), (2039, 2014, N'venv');

-- Department: System Development, Track: Data Engineering, Course: Advanced SQL & Data Warehousing
INSERT INTO Question_Choice (Question_Choice_ID, Question_ID, Choice_Text) VALUES (2040, 2015, N'SUM()'), (2041, 2015, N'COUNT()'), (2042, 2015, N'ROW_NUMBER()'), (2043, 2015, N'WHERE'), (2044, 2017, N'Complex Table Engine'), (2045, 2017, N'Common Table Expression'), (2046, 2017, N'Continuous Table Entry'), (2047, 2017, N'Cascading Table Expression'), (2048, 2018, N'Dimension Table'), (2049, 2018, N'Staging Table'), (2050, 2018, N'Fact Table'), (2051, 2018, N'View'), (2052, 2020, N'Star Schema'), (2053, 2020, N'Galaxy Schema'), (2054, 2020, N'Fact Constellation Schema'), (2055, 2020, N'Snowflake Schema'), (2056, 2021, N'HAVING'), (2057, 2021, N'WHERE'), (2058, 2021, N'FILTER'), (2059, 2021, N'QUALIFY'), (2060, 2023, N'A primary key from the source system'), (2061, 2023, N'A foreign key'), (2062, 2023, N'A system-generated unique identifier'), (2063, 2023, N'A composite key'), (2064, 2024, N'Rapidly Changing Dimension (RCD)'), (2065, 2024, N'Dimension History Table (DHT)'), (2066, 2024, N'Slowly Changing Dimension (SCD)'), (2067, 2024, N'Fact Change Log (FCL)'), (2068, 2026, N'INNER JOIN'), (2069, 2026, N'LEFT JOIN'), (2070, 2026, N'FULL OUTER JOIN'), (2071, 2026, N'RIGHT JOIN'), (2072, 2028, N'Hierarchical Model'), (2073, 2028, N'Network Model'), (2074, 2028, N'Relational Model'), (2075, 2028, N'Object-Oriented Model'), (2076, 2029, N'Online Application Processing'), (2077, 2029, N'Online Analytical Processing'), (2078, 2029, N'Operational Link Analysis Protocol'), (2079, 2029, N'Online Transactional Processing');

-- Department: System Development, Track: Data Engineering, Course: ETL, Hadoop & Spark
INSERT INTO Question_Choice (Question_Choice_ID, Question_ID, Choice_Text) VALUES (2080, 2030, N'Extract, Transform, Load'), (2081, 2030, N'Execute, Test, Load'), (2082, 2030, N'Extract, Test, Link'), (2083, 2030, N'Execute, Transform, Link'), (2084, 2032, N'YARN'), (2085, 2032, N'MapReduce'), (2086, 2032, N'HDFS'), (2087, 2032, N'Spark Core'), (2088, 2033, N'RDD'), (2089, 2033, N'DataFrame'), (2090, 2033, N'Schema'), (2091, 2033, N'Tuple'), (2092, 2035, N'HDFS'), (2093, 2035, N'Spark'), (2094, 2035, N'YARN'), (2095, 2035, N'MapReduce'), (2096, 2036, N'Actions are executed before transformations'), (2097, 2036, N'All code is compiled at runtime'), (2098, 2036, N'Transformations are not executed until an action is called'), (2099, 2036, N'Spark evaluates code line by line'), (2100, 2038, N'filter()'), (2101, 2038, N'map()'), (2102, 2038, N'count()'), (2103, 2038, N'select()'), (2104, 2039, N'Fast, random reads and writes'), (2105, 2039, N'Storing small files'), (2106, 2039, N'Transactional operations'), (2107, 2039, N'Storing large files'), (2108, 2041, N'To store data on worker nodes'), (2109, 2041, N'To run individual tasks'), (2110, 2041, N'To coordinate workers and execute the main function'), (2111, 2041, N'To manage cluster resources'), (2112, 2043, N'collect()'), (2113, 2043, N'filter()'), (2114, 2043, N'take()'), (2115, 2043, N'save()'), (2116, 2044, N'Test'), (2117, 2044, N'Transform'), (2118, 2044, N'Table'), (2119, 2044, N'Terminate');

-- Department: System Development, Track: Data Engineering, Course: Cloud Data Platforms & Data Lakes
INSERT INTO Question_Choice (Question_Choice_ID, Question_ID, Choice_Text) VALUES (2120, 2045, N'Data Warehouse'), (2121, 2045, N'Data Mart'), (2122, 2045, N'Data Lake'), (2123, 2045, N'Database'), (2124, 2047, N'CSV'), (2125, 2047, N'JSON'), (2126, 2047, N'Parquet'), (2127, 2047, N'XML'), (2128, 2048, N'Amazon RDS'), (2129, 2048, N'Amazon EC2'), (2130, 2048, N'Amazon S3'), (2131, 2048, N'Amazon DynamoDB'), (2132, 2050, N'Google Cloud Storage'), (2133, 2050, N'Google Bigtable'), (2134, 2050, N'Google BigQuery'), (2135, 2050, N'Google Dataflow'), (2136, 2051, N'Human readability'), (2137, 2051, N'Improved query performance and compression'), (2138, 2051, N'Ease of manual editing'), (2139, 2051, N'Better support for nested data'), (2140, 2053, N'Azure SQL Database'), (2141, 2053, N'Azure Cosmos DB'), (2142, 2053, N'Azure Synapse Analytics'), (2143, 2053, N'Azure Data Lake Storage'), (2144, 2054, N'Data'), (2145, 2054, N'Schema'), (2146, 2054, N'Payload'), (2147, 2054, N'Format'), (2148, 2056, N'JSON'), (2149, 2056, N'CSV'), (2150, 2056, N'XML'), (2151, 2056, N'ORC'), (2152, 2058, N'The service is free to use'), (2153, 2058, N'The service runs without servers'), (2154, 2058, N'Users do not manage the underlying infrastructure'), (2155, 2058, N'The service has no performance limits'), (2156, 2059, N'Amazon S3'), (2157, 2059, N'Amazon Redshift'), (2158, 2059, N'Amazon Aurora'), (2159, 2059, N'Amazon DynamoDB');

-- Department: System Development, Track: Data Engineering, Course: Real-time Streaming with Kafka
INSERT INTO Question_Choice (Question_Choice_ID, Question_ID, Choice_Text) VALUES (2160, 2060, N'A relational database'), (2161, 2060, N'A distributed streaming platform'), (2162, 2060, N'A workflow scheduler'), (2163, 2060, N'An in-memory data grid'), (2164, 2062, N'Queue'), (2165, 2062, N'Topic'), (2166, 2062, N'Log'), (2167, 2062, N'Table'), (2168, 2063, N'To write code'), (2169, 2063, N'To consume data'), (2170, 2063, N'To manage the cluster'), (2171, 2063, N'To store data'), (2172, 2065, N'Topics'), (2173, 2065, N'Offsets'), (2174, 2065, N'Partitions'), (2175, 2065, N'Brokers'), (2176, 2066, N'To store topic data'), (2177, 2066, N'To process streaming data'), (2178, 2066, N'To manage cluster metadata and broker coordination'), (2179, 2066, N'To provide a user interface'), (2180, 2068, N'Brokers'), (2181, 2068, N'Topics'), (2182, 2068, N'Partitions'), (2183, 2068, N'Offsets'), (2184, 2069, N'Key'), (2185, 2069, N'Index'), (2186, 2069, N'Offset'), (2187, 2069, N'Watermark'), (2188, 2071, N'Producer'), (2189, 2071, N'Consumer Group'), (2190, 2071, N'Zookeeper'), (2191, 2071, N'Broker'), (2192, 2073, N'Strong consistency'), (2193, 2073, N'Support for complex queries'), (2194, 2073, N'High throughput and low latency'), (2195, 2073, N'Easy data transformation'), (2196, 2074, N'Group'), (2197, 2074, N'Swarm'), (2198, 2074, N'Cluster'), (2199, 2074, N'Federation');

-- Department: System Development, Track: Data Engineering, Course: Workflow Orchestration with Airflow
INSERT INTO Question_Choice (Question_Choice_ID, Question_ID, Choice_Text) VALUES (2200, 2075, N'Pipeline'), (2201, 2075, N'Flowchart'), (2202, 2075, N'DAG'), (2203, 2075, N'Script'), (2204, 2077, N'Data Analytics Graph'), (2205, 2077, N'Directed Acyclic Graph'), (2206, 2077, N'Data Automation Grid'), (2207, 2077, N'Direct Action Graph'), (2208, 2078, N'Node'), (2209, 2078, N'Step'), (2210, 2078, N'Job'), (2211, 2078, N'Task'), (2212, 2080, N'Executor'), (2213, 2080, N'Webserver'), (2214, 2080, N'Scheduler'), (2215, 2080, N'Worker'), (2216, 2081, N'Atomicity'), (2217, 2081, N'Idempotency'), (2218, 2081, N'Durability'), (2219, 2081, N'Concurrency'), (2220, 2083, N'BashOperator'), (2221, 2083, N'PythonOperator'), (2222, 2083, N'FunctionOperator'), (2223, 2083, N'ScriptOperator'), (2224, 2084, N'Backfilling'), (2225, 2084, N'Re-running'), (2226, 2084, N'Catching up'), (2227, 2084, N'Time-traveling'), (2228, 2086, N'A type of operator'), (2229, 2086, N'A connection string'), (2230, 2086, N'A mechanism for tasks to pass messages'), (2231, 2086, N'A user-facing variable'), (2232, 2088, N'Variables'), (2233, 2088, N'Operators'), (2234, 2088, N'Connections'), (2235, 2088, N'Dependencies'), (2236, 2089, N'Hook'), (2237, 2089, N'Provider'), (2238, 2089, N'Operator'), (2239, 2089, N'Plugin');

-- Department: System Development, Track: Data Engineering, Course: Data Engineering Capstone Project
INSERT INTO Question_Choice (Question_Choice_ID, Question_ID, Choice_Text) VALUES (2240, 2090, N'Choosing a cloud provider'), (2241, 2090, N'Writing the transformation logic'), (2242, 2090, N'Understanding requirements and data sources'), (2243, 2090, N'Setting up monitoring'), (2244, 2092, N'To make the pipeline run faster'), (2245, 2092, N'To reduce data storage costs'), (2246, 2092, N'To ensure data is accurate, complete, and reliable'), (2247, 2092, N'To simplify the pipeline code'), (2248, 2093, N'Code readability'), (2249, 2093, N'Scalability'), (2250, 2093, N'The programming language used'), (2251, 2093, N'The number of developers'), (2252, 2095, N'To archive old data'), (2253, 2095, N'To serve data to end-users'), (2254, 2095, N'To temporarily store data during transformation'), (2255, 2095, N'To run analytical queries'), (2256, 2096, N'The brand name'), (2257, 2096, N'How easy it is to install'), (2258, 2096, N'The data model and query patterns'), (2259, 2096, N'The popularity on social media'), (2260, 2098, N'Data Transformation'), (2261, 2098, N'Data Governance'), (2262, 2098, N'Data Orchestration'), (2263, 2098, N'Data Science'), (2264, 2099, N'Lack of data'), (2265, 2099, N'Handling evolving data schemas'), (2266, 2099, N'Hardware being too fast'), (2267, 2099, N'Too much documentation'), (2268, 2101, N'Complex and rigid'), (2269, 2101, N'Written by a single person'), (2270, 2101, N'Modular and maintainable'), (2271, 2101, N'Quick to build initially'), (2272, 2103, N'It runs very fast'), (2273, 2103, N'It is cheap to operate'), (2274, 2103, N'It never fails'), (2275, 2103, N'It can recover from failures'), (2276, 2104, N'To increase data volume'), (2277, 2104, N'To slow down the pipeline'), (2278, 2104, N'For debugging and monitoring'), (2279, 2104, N'To meet regulatory requirements');